{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87095728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "#have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "#jobs data.\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpage https://www.shine.com/\n",
    "#2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "#3. Then click the searchbutton.\n",
    "#4. Then scrape the data for the first 10 jobs results you get.\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "#Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bffb012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9312cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#Step 1:get the webpage\n",
    "base_url = \"http://www.shine.com/\"\n",
    "search_url = base_url +  \"job-search/data-analyst-jobs-in-bangalore\"\n",
    "\n",
    "#Step2: enter search parameters and click search button\n",
    "\n",
    "params = {\n",
    "    \"q\": 'Data Analyst',\n",
    "    'I': 'Bangalore'\n",
    "}    \n",
    "response = requests.get(search_url, params=params)\n",
    "soup=BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "#step4: scrap data for the first 10 jobs\n",
    "jobs = []\n",
    "job_results = soup.find_all(\"div\", class_=\"search_listing\")\n",
    "for job_result in job_results[:10]:\n",
    "    job_title = job_result.find(\"h2\").text.strip()     \n",
    "    company_name = job_result.find(\"span\", class_=\"result-display__profile__name\").text.strip()\n",
    "    job_location = job_result.find(\"span\", class_=\"result-display__location\").text.strip()\n",
    "    experience_required = job_result.find(\"span\", class_=\"result-display__experience\").text.strip()\n",
    "    jobs.append({\n",
    "        \"Job Title\": job_title,\n",
    "        \"Company Name\": company_name,\n",
    "        \"Job Location\": job_location,\n",
    "        \"Experience Required\": experience_required\n",
    "    })\n",
    "    \n",
    "#Step 5:Create a dataframe\n",
    "df=pd.DataFrame(jobs)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a157b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2:Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You\n",
    "#have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpage https://www.shine.com/\n",
    "#2. Enter “Data Scientist” in “Job title, Skills” field and enter “Bangalore” in “enter thelocation” field.\n",
    "#3. Then click the search button.\n",
    "#4. Then scrape the data for the first 10 jobs results you get.\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "#Note: All of the above steps have to be done in code. No step is to be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bc8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898c79d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Set the search parameters\n",
    "job_title = \"Data Scientist\"\n",
    "location = \"Bangalore\"\n",
    "\n",
    "# Step 2: Create the search URL with parameters\n",
    "search_url = f\"https://www.shine.com/job-search/{job_title}-jobs-in-{location}\"\n",
    "\n",
    "# Step 3: Send a GET request to the search URL\n",
    "response = requests.get(search_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 4: Scrape data for the first 10 jobs\n",
    "jobs_data = []\n",
    "job_results = soup.find_all('li', class_='search_listing')\n",
    "\n",
    "for job_result in job_results[:10]:\n",
    "    job_title = job_result.find('h3', class_='job_title').text.strip()\n",
    "    job_location = job_result.find('span', class_='loc').text.strip()\n",
    "    company_name = job_result.find('span', class_='company_name').text.strip()\n",
    "\n",
    "    jobs_data.append({\n",
    "        'Job Title': job_title,\n",
    "        'Job Location': job_location,\n",
    "        'Company Name': company_name\n",
    "    })\n",
    "\n",
    "# Step 5: Create a DataFrame\n",
    "jobs_df = pd.DataFrame(jobs_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(jobs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d24e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3: In this question you have to scrape data using the filters available on the webpage\n",
    "#You have to use the location and salary filter.\n",
    "#You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "#You have to scrape the job-title, job-location, company name, experience required.\n",
    "#The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "#The task will be done as shown in the below steps:\n",
    "#1. first get the web page https://www.shine.com/\n",
    "#2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "#3. Then click the search button.\n",
    "#4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "#5. Then scrape the data for the first 10 jobs results you get.\n",
    "#6. Finally create a dataframe of the scrapeddata.\n",
    "#Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "553ce5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the search results page\n",
    "base_url = \"https://www.shine.com\"\n",
    "search_url = f\"{base_url}/job-search/data-scientist-jobs\"\n",
    "response = requests.get(search_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 2-4: No need to input search terms manually, as we're directly accessing the search results page.\n",
    "\n",
    "# Step 5: Scrape data for the first 10 job results\n",
    "job_data = []\n",
    "results = soup.find_all('div', {'class': 'w-90'})\n",
    "\n",
    "for result in results[:10]:\n",
    "    title = result.find('li', {'class': 'result-display__profile__title'})\n",
    "    location = result.find('li', {'class': 'result-display__profile__company-location'})\n",
    "    company = result.find('li', {'class': 'result-display__profile__company-name'})\n",
    "    experience = result.find('li', {'class': 'result-display__profile__experience'})\n",
    "\n",
    "    job_data.append({\n",
    "        'Title': title.get_text(strip=True) if title else 'N/A',\n",
    "        'Location': location.get_text(strip=True) if location else 'N/A',\n",
    "        'Company': company.get_text(strip=True) if company else 'N/A',\n",
    "        'Experience': experience.get_text(strip=True) if experience else 'N/A'\n",
    "    })\n",
    "\n",
    "# Step 6: Create a DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "484f5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "#6. Brand\n",
    "#7. ProductDescription\n",
    "#8. Price\n",
    "#The attributes which you have to scrape is ticked marked in the below image.\n",
    "#To scrape the data you have to go through following steps:\n",
    "#1. Go to Flipkart webpage by url :https://www.flipkart.com/\n",
    "#2. Enter “sunglasses” in the search fieldwhere “search for products, brands and more” is written and\n",
    "#click the search icon\n",
    "#3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "#required data as usual.\n",
    "#4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "#click on it.\n",
    "#5. Now scrape data from this page as usual\n",
    "#6. Repeat this until you get data for 100sunglasses.\n",
    "#Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a1e27c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Brand                                 ProductDescription Price\n",
      "0             N/A                                                N/A   N/A\n",
      "1             N/A                                                N/A   N/A\n",
      "2             N/A                                                N/A   N/A\n",
      "3             N/A                                                N/A   N/A\n",
      "4     Eyewearlabs  Polarized, UV Protection Rectangular Sunglasse...  2199\n",
      "5       Elligator  UV Protection Cat-eye, Retro Square, Oval, Rou...   179\n",
      "6        Fastrack   UV Protection Rectangular Sunglasses (Free Size)   629\n",
      "7            BKGE  Polarized, UV Protection Retro Square Sunglass...   197\n",
      "8       ROYAL SON  Polarized, UV Protection Retro Square Sunglass...   594\n",
      "9      style guru  Mirrored, Night Vision, UV Protection Wayfarer...   771\n",
      "10         RBGIIT  Night Vision, UV Protection, Polarized Wrap-ar...   225\n",
      "11        eyedens   UV Protection Rectangular Sunglasses (Free Size)   349\n",
      "12  VINCENT CHASE             UV Protection Wayfarer Sunglasses (59)   569\n",
      "13           IDEE          UV Protection Rectangular Sunglasses (58)   901\n",
      "14            N/A                                                N/A   N/A\n",
      "15            N/A                                                N/A   N/A\n",
      "16            N/A                                                N/A  1799\n",
      "17            N/A                                                N/A   N/A\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Base URL for Flipkart's search results\n",
    "base_url = \"https://www.flipkart.com\"\n",
    "\n",
    "# Start with the first page\n",
    "search_url = \"https://www.flipkart.com/search?q=sunglasses\"\n",
    "response = requests.get(search_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Define a function to scrape data from a page\n",
    "def scrape_page(page_soup):\n",
    "    results = page_soup.find_all('div', {'class': '_1AtVbE'})\n",
    "    for result in results:\n",
    "        brand_elem = result.find('div', {'class': '_2WkVRV'})\n",
    "        description_elem = result.find('a', {'class': 'IRpwTa'})\n",
    "        price_elem = result.find('div', {'class': '_30jeq3'})\n",
    "        \n",
    "        brand = brand_elem.text if brand_elem else 'N/A'\n",
    "        description = description_elem.text if description_elem else 'N/A'\n",
    "        price = price_elem.text.replace('₹','').replace(',','') if price_elem else 'N/A'\n",
    "        \n",
    "        brands.append(brand)\n",
    "        descriptions.append(description)\n",
    "        prices.append(price)\n",
    "\n",
    "# Scrape data from the current page\n",
    "scrape_page(soup)\n",
    "\n",
    "# Loop to navigate through pages and scrape data\n",
    "while len(brands) < 100:\n",
    "    next_button = soup.find('a', {'class': '_1LKTO3', 'rel': 'next'})\n",
    "    if not next_button:\n",
    "        break\n",
    "    \n",
    "    next_url = base_url + next_button['href']\n",
    "    response = requests.get(next_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    scrape_page(soup)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    'Brand': brands[:100],\n",
    "    'ProductDescription': descriptions[:100],\n",
    "    'Price': prices[:100]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "#https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "#place=FLIPKART\n",
    "#As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "#1. Rating\n",
    "#2. Review summary\n",
    "#3. Full review\n",
    "#4. You have to scrape this data for first 100reviews.\n",
    "#Note: All the steps required during scraping should be done through code only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6fc6216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Rating, ReviewSummary, FullReview]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "# Base URL for the reviews page\n",
    "base_url = \"https://www.flipkart.com\"\n",
    "\n",
    "# The given URL for iPhone 11 reviews\n",
    "reviews_url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "# Fetch the reviews page\n",
    "response = requests.get(reviews_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Function to scrape reviews from the current page\n",
    "def scrape_reviews(page_soup):\n",
    "    review_containers = page_soup.find_all('div', {'class': '_27M-vq'})\n",
    "\n",
    "    for container in review_containers:\n",
    "        rating = container.find('div', {'class': '_3LWZlK _1BLPMq'}).text\n",
    "        summary = container.find('p', {'class': '_2-N8zT'}).text\n",
    "        full_review = container.find('div', {'class': 't-ZTKy'}).text\n",
    "\n",
    "        ratings.append(rating)\n",
    "        review_summaries.append(summary)\n",
    "        full_reviews.append(full_review)\n",
    "\n",
    "# Scrape reviews from the current page\n",
    "scrape_reviews(soup)\n",
    "\n",
    "# Loop to navigate through pages and scrape data\n",
    "while len(ratings) < 100:\n",
    "    next_button = soup.find('a', {'class': '_1LKTO3', 'rel': 'next'})\n",
    "    if not next_button:\n",
    "        break\n",
    "    \n",
    "    next_url = base_url + next_button['href']\n",
    "    response = requests.get(next_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    scrape_reviews(soup)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    'Rating': ratings[:100],\n",
    "    'ReviewSummary': review_summaries[:100],\n",
    "    'FullReview': full_reviews[:100]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c8a4f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6: Scrape data forfirst 100 sneakers you find whenyou visit flipkart.com and search for “sneakers” inthe\n",
    "#search field.\n",
    "#You have to scrape 3 attributes of each sneaker:\n",
    "#1. Brand\n",
    "#2. ProductDescription\n",
    "#3. Price\n",
    "#As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4992993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Brand                                 ProductDescription Price\n",
      "0    N/A                                                N/A   N/A\n",
      "1    N/A                                                N/A   N/A\n",
      "2    N/A  Modern Trendy Sneakers boot Sneakers Sneakers ...   449\n",
      "3    N/A                                   Sneakers For Men   299\n",
      "4    N/A                         Hustle V2 Sneakers For Men  1299\n",
      "5    N/A  Stylish and Comfirtable Shoes For Mens Sneaker...   584\n",
      "6    N/A                                 Sneakers For Women   499\n",
      "7    N/A    Mens Stylish, Partywear Casual Sneakers For Men   539\n",
      "8    N/A                                   Sneakers For Men   435\n",
      "9    N/A  Trendy-21 White Color Change Sneakers,Casuals,...   649\n",
      "10   N/A  Bersache Sports Walking Gym sneakers Trekking ...   929\n",
      "11   N/A                                 Sneakers For Women   377\n",
      "12   N/A                                                N/A   N/A\n",
      "13   N/A                                                N/A   N/A\n",
      "14   N/A                                                N/A   669\n",
      "15   N/A                                                N/A   N/A\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Base URL for Flipkart's search results\n",
    "base_url = \"https://www.flipkart.com\"\n",
    "\n",
    "# Start with the first page\n",
    "search_url = \"https://www.flipkart.com/search?q=sneakers\"\n",
    "response = requests.get(search_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Function to scrape data from a page\n",
    "def scrape_page(page_soup):\n",
    "    results = page_soup.find_all('div', {'class': '_1AtVbE'})\n",
    "    for result in results:\n",
    "        brand_elem = result.find('div', {'class': '_2B_pmu'})\n",
    "        description_elem = result.find('a', {'class': 'IRpwTa'})\n",
    "        price_elem = result.find('div', {'class': '_30jeq3'})\n",
    "        \n",
    "        brand = brand_elem.text if brand_elem else 'N/A'\n",
    "        description = description_elem.text if description_elem else 'N/A'\n",
    "        price = price_elem.text.replace('₹','').replace(',','') if price_elem else 'N/A'\n",
    "        \n",
    "        brands.append(brand)\n",
    "        descriptions.append(description)\n",
    "        prices.append(price)\n",
    "\n",
    "# Scrape data from the current page\n",
    "scrape_page(soup)\n",
    "\n",
    "# Loop to navigate through pages and scrape data\n",
    "while len(brands) < 100:\n",
    "    next_button = soup.find('a', {'class': '_1LKTO3', 'rel': 'next'})\n",
    "    if not next_button:\n",
    "        break\n",
    "    \n",
    "    next_url = base_url + next_button['href']\n",
    "    response = requests.get(next_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    scrape_page(soup)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    'Brand': brands[:100],\n",
    "    'ProductDescription': descriptions[:100],\n",
    "    'Price': prices[:100]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9ab5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "#set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "#After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "#1. Title\n",
    "#2. Ratings\n",
    "#3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e14eead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Ratings, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "titles = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "# Base URL for Amazon India's search results\n",
    "base_url = \"https://www.amazon.in\"\n",
    "\n",
    "# Search URL for laptops\n",
    "search_url = \"https://www.amazon.in/s?k=laptop\"\n",
    "\n",
    "# Set headers to mimic a browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Perform the search with filters\n",
    "response = requests.get(search_url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Set CPU Type filter to \"Intel Core i7\"\n",
    "filter_url = None\n",
    "for link in soup.find_all('a', {'class': 'a-link-normal', 'aria-label': 'Intel Core i7'}):\n",
    "    if 'href' in link.attrs:\n",
    "        filter_url = link['href']\n",
    "        break\n",
    "\n",
    "# If filter is found, apply it and get the filtered page\n",
    "if filter_url:\n",
    "    filter_url = base_url + filter_url\n",
    "    response = requests.get(filter_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Function to scrape data from a page\n",
    "def scrape_page(page_soup):\n",
    "    results = page_soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    for result in results[:10]:\n",
    "        title = result.find('h2', {'class': 'a-size-mini'}).text.strip()\n",
    "        rating = result.find('span', {'class': 'a-icon-alt'}).text.split()[0] if result.find('span', {'class': 'a-icon-alt'}) else 'N/A'\n",
    "        price = result.find('span', {'class': 'a-price-whole'}).text.replace(',', '') if result.find('span', {'class': 'a-price-whole'}) else 'N/A'\n",
    "        \n",
    "        titles.append(title)\n",
    "        ratings.append(rating)\n",
    "        prices.append(price)\n",
    "\n",
    "# Scrape data from the current page\n",
    "scrape_page(soup)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    'Title': titles,\n",
    "    'Ratings': ratings,\n",
    "    'Price': prices\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a724bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "#The above task will be done in following steps:\n",
    "#1. First get the webpagehttps://www.azquotes.com/\n",
    "#2. Click on TopQuotes\n",
    "#3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a537a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb258d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d65f686",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1241\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1243\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1099\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    490\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m             retries = retries.increment(\n\u001b[0m\u001b[0;32m    788\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    770\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1241\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1243\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1099\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3212\\3357384875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquotes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mnext_page_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{top_quotes_url}/{page}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_page_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time  \n",
    "# Import the time module\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "quotes = []\n",
    "authors = []\n",
    "types = []\n",
    "\n",
    "# Base URL for the website\n",
    "base_url = \"https://www.azquotes.com\"\n",
    "\n",
    "# Step 1: Get the main page\n",
    "main_page_url = \"https://www.azquotes.com/\"\n",
    "response = requests.get(main_page_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 2: Click on Top Quotes\n",
    "top_quotes_link = soup.find('a', text='Top Quotes')['href']\n",
    "top_quotes_url = base_url + top_quotes_link\n",
    "response = requests.get(top_quotes_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Function to scrape data from a page\n",
    "def scrape_page(page_soup):\n",
    "    results = page_soup.find_all('div', {'class': 'boxy'})\n",
    "    for result in results:\n",
    "        quote = result.find('a', {'class': 'title'})['title']\n",
    "        author = result.find('div', {'class': 'author'}).text\n",
    "        quote_type = result.find('div', {'class': 'kw-box'}).text if result.find('div', {'class': 'kw-box'}) else 'N/A'\n",
    "        \n",
    "        quotes.append(quote)\n",
    "        authors.append(author)\n",
    "        types.append(quote_type)\n",
    "\n",
    "# Scrape data from the current page\n",
    "scrape_page(soup)\n",
    "\n",
    "# Loop to navigate through pages and scrape data\n",
    "page = 2  # Starting from the second page since the first page is already scraped\n",
    "while len(quotes) < 1000:\n",
    "    next_page_url = f\"{top_quotes_url}/{page}\"\n",
    "    response = requests.get(next_page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    scrape_page(soup)\n",
    "    page += 1\n",
    "\n",
    "    # Pause for a moment to avoid overwhelming the server\n",
    "    time.sleep(1)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "data = {\n",
    "    'Quote': quotes[:1000],\n",
    "    'Author': authors[:1000],\n",
    "    'Type': types[:1000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419ebf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "#Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpagehttps://www.jagranjosh.com/\n",
    "#2. Then You have to click on the GK option\n",
    "#3. Then click on the List of all Prime Ministers of India\n",
    "#4. Then scrap the mentioned data and make theDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c7d2506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found. Website structure might have changed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.jagranjosh.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 2: Click on the GK option\n",
    "gk_link = soup.find(\"a\", text=\"GK\")\n",
    "gk_url = gk_link[\"href\"]\n",
    "gk_response = requests.get(gk_url)\n",
    "gk_soup = BeautifulSoup(gk_response.content, \"html.parser\")\n",
    "\n",
    "# Step 3: Click on the List of all Prime Ministers of India\n",
    "pm_link = gk_soup.find(\"a\", text=\"List of all Prime Ministers of India\")\n",
    "pm_url = pm_link[\"href\"]\n",
    "pm_response = requests.get(pm_url)\n",
    "pm_soup = BeautifulSoup(pm_response.content, \"html.parser\")\n",
    "\n",
    "# Step 4: Scrape the data and create a DataFrame\n",
    "pm_data = []\n",
    "table = pm_soup.find(\"table\", class_=\"table4\")\n",
    "\n",
    "if table:\n",
    "    rows = table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "\n",
    "    for row in rows:\n",
    "        columns = row.find_all(\"td\")\n",
    "        name = columns[0].get_text()\n",
    "        born_dead = columns[1].get_text()\n",
    "        term = columns[2].get_text()\n",
    "        remarks = columns[3].get_text()\n",
    "        pm_data.append([name, born_dead, term, remarks])\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(pm_data, columns=[\"Name\", \"Born-Dead\", \"Term of office\", \"Remarks\"])\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Table not found. Website structure might have changed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f36e5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "#Car name and Price) from https://www.motor1.com/\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpage https://www.motor1.com/\n",
    "#2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "#3. Then click on 50 most expensive carsin the world..\n",
    "#4. Then scrap the mentioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f09be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search input not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.motor1.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 2: Find the search bar and input the query\n",
    "search_query = \"50 most expensive cars\"\n",
    "search_input = soup.find(\"input\", {\"type\": \"search\"})\n",
    "if search_input:\n",
    "    search_input[\"value\"] = search_query\n",
    "\n",
    "    # Submit the form (if available)\n",
    "    search_form = search_input.find_parent(\"form\")\n",
    "    if search_form:\n",
    "        search_url = search_form[\"action\"]\n",
    "        search_response = requests.get(search_url, params={\"s\": search_query})\n",
    "        search_soup = BeautifulSoup(search_response.text, 'html.parser')\n",
    "\n",
    "        # Step 3: Find and click on the link for '50 most expensive cars' if available\n",
    "        link_to_click = search_soup.find(\"a\", text=\"50 Most Expensive Cars In The World\")\n",
    "        if link_to_click:\n",
    "            target_url = link_to_click[\"href\"]\n",
    "            cars_response = requests.get(target_url)\n",
    "            cars_soup = BeautifulSoup(cars_response.text, 'html.parser')\n",
    "\n",
    "            # Step 4: Scrap the data and create a dataframe\n",
    "            car_data = []\n",
    "            car_elements = cars_soup.find_all(\"div\", class_=\"motor1-car-item\")\n",
    "            for car_element in car_elements:\n",
    "                car_name = car_element.find(\"h3\", class_=\"motor1-car-title\").text.strip()\n",
    "                car_price = car_element.find(\"div\", class_=\"motor1-car-price\").text.strip()\n",
    "                car_data.append({\"Car Name\": car_name, \"Price\": car_price})\n",
    "\n",
    "            df = pd.DataFrame(car_data)\n",
    "            print(df)\n",
    "\n",
    "        else:\n",
    "            print(\"Link for '50 Most Expensive Cars In The World' not found.\")\n",
    "    else:\n",
    "        print(\"Search form not found.\")\n",
    "else:\n",
    "    print(\"Search input not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb15bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323dc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
